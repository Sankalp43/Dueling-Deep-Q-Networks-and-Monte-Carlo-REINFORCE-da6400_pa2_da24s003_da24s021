{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium -qU\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-14T09:13:19.827287Z",
     "iopub.status.busy": "2025-04-14T09:13:19.827048Z",
     "iopub.status.idle": "2025-04-14T09:13:29.626519Z",
     "shell.execute_reply": "2025-04-14T09:13:29.625470Z",
     "shell.execute_reply.started": "2025-04-14T09:13:19.827266Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:596: UserWarning: \u001b[33mWARN: plugin: shimmy.registration:register_gymnasium_envs raised Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py\", line 594, in load_plugin_envs\n",
      "    fn()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 304, in register_gymnasium_envs\n",
      "    _register_atari_envs()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/shimmy/registration.py\", line 205, in _register_atari_envs\n",
      "    import ale_py\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/__init__.py\", line 68, in <module>\n",
      "    register_v0_v4_envs()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 178, in register_v0_v4_envs\n",
      "    _register_rom_configs(legacy_games, obs_types, versions)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/ale_py/registration.py\", line 63, in _register_rom_configs\n",
      "    gymnasium.register(\n",
      "    ^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: partially initialized module 'gymnasium' has no attribute 'register' (most likely due to a circular import)\n",
      "\u001b[0m\n",
      "  logger.warn(f\"plugin: {plugin.value} raised {traceback.format_exc()}\")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import uuid\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the programming assignment 2, we are required to implement two alogrithms, `Monte Carlo - REINFORCE` and `Dueling Deep Q-learning Network (Dueling DQN)`. After wards, the goal is to run hyperparameter search to find the best hyperparameters for both algorithms on two environments, `CartPole-v1` and `Acrobot-v2`. \n",
    "\n",
    "In this jupyter notebook let's implement the `Monte Carlo - REINFORCE` algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T09:13:29.628176Z",
     "iopub.status.busy": "2025-04-14T09:13:29.627853Z",
     "iopub.status.idle": "2025-04-14T09:13:29.634923Z",
     "shell.execute_reply": "2025-04-14T09:13:29.633960Z",
     "shell.execute_reply.started": "2025-04-14T09:13:29.628153Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dims):\n",
    "        \"\"\"\n",
    "        Network with configurable depth and layer sizes\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimension of input state\n",
    "            output_dim: Dimension of output action space\n",
    "            hidden_dims: List of hidden layer dimensions\n",
    "        \"\"\"\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        # Build network with arbitrary depth based on hidden_dims list\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "\n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_probs = self.network(state)\n",
    "        return torch.distributions.Categorical(probs=action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T09:13:29.636388Z",
     "iopub.status.busy": "2025-04-14T09:13:29.635920Z",
     "iopub.status.idle": "2025-04-14T09:13:29.669565Z",
     "shell.execute_reply": "2025-04-14T09:13:29.668570Z",
     "shell.execute_reply.started": "2025-04-14T09:13:29.636347Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Baseline Value Network for estimating V(s; Î¦).\n",
    "    Used in the 'with baseline' version of MC-REINFORCE.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dims):\n",
    "        \"\"\"\n",
    "        Network with configurable depth and layer sizes\n",
    "\n",
    "        Args:\n",
    "            input_dim: Dimension of input state\n",
    "            hidden_dims: List of hidden layer dimensions\n",
    "        \"\"\"\n",
    "        super(ValueNetwork, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "\n",
    "        # Build network with arbitrary depth based on hidden_dims list\n",
    "        for dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = dim\n",
    "\n",
    "        # Output layer (single value)\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.network(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T09:13:29.671700Z",
     "iopub.status.busy": "2025-04-14T09:13:29.671330Z",
     "iopub.status.idle": "2025-04-14T09:13:29.695574Z",
     "shell.execute_reply": "2025-04-14T09:13:29.694686Z",
     "shell.execute_reply.started": "2025-04-14T09:13:29.671673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n",
    "    def __init__(self, env, hyperparams):\n",
    "        self.env = env\n",
    "        self.hp = hyperparams\n",
    "\n",
    "        # Initialize policy network\n",
    "        # Initialize policy network\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "\n",
    "        # Create hidden layers configuration based on parameters\n",
    "        if 'layer_sizes' in self.hp:\n",
    "            # Use the list of layer sizes directly\n",
    "            hidden_dims = self.hp['layer_sizes']\n",
    "        elif 'network_depth' in self.hp and 'layer_size' in self.hp:\n",
    "            # Backward compatibility - use same size for all layers\n",
    "            hidden_dims = [self.hp['layer_size']] * self.hp['network_depth']\n",
    "        else:\n",
    "            # Fallback to original implementation\n",
    "            hidden_dims = [self.hp['hidden_dim']]\n",
    "\n",
    "\n",
    "        self.policy_net = PolicyNetwork(state_dim, action_dim, hidden_dims)\n",
    "        self.optimizer_policy = optim.Adam(self.policy_net.parameters(), lr=self.hp['lr'])\n",
    "\n",
    "        # Setup learning rate decay scheduler if configured\n",
    "        if 'lr_decay' in self.hp and self.hp['lr_decay'] < 1.0:\n",
    "            self.scheduler_policy = optim.lr_scheduler.ExponentialLR(\n",
    "                self.optimizer_policy, gamma=self.hp['lr_decay']\n",
    "            )\n",
    "        else:\n",
    "            self.scheduler_policy = None\n",
    "\n",
    "        # Initialize value network (only for 'with baseline' version)\n",
    "        if self.hp['use_baseline']:\n",
    "            self.value_net = ValueNetwork(state_dim, hidden_dims)\n",
    "            self.optimizer_value = optim.Adam(self.value_net.parameters(), lr=self.hp['lr_value'])\n",
    "\n",
    "            # Setup value network learning rate decay if configured\n",
    "            if 'lr_decay' in self.hp and self.hp['lr_decay'] < 1.0:\n",
    "                self.scheduler_value = optim.lr_scheduler.ExponentialLR(\n",
    "                    self.optimizer_value, gamma=self.hp['lr_decay']\n",
    "                )\n",
    "            else:\n",
    "                self.scheduler_value = None\n",
    "\n",
    "        # Reward tracking\n",
    "        self.reward_buffer = deque(maxlen=100)\n",
    "        self.avg_reward_list = []\n",
    "        self.episode_reward = []\n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(np.array(state))\n",
    "        dist = self.policy_net(state)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def update_policy(self, rewards, log_probs, states=None):\n",
    "        \"\"\"\n",
    "        Policy update method for both versions (with and without baseline).\n",
    "        \"\"\"\n",
    "        returns = self._compute_returns(rewards)\n",
    "\n",
    "        policy_loss = []\n",
    "\n",
    "        if self.hp['use_baseline']:\n",
    "            # Update baseline using TD(0) method\n",
    "            for i in range(len(states)):\n",
    "                state_value = self.value_net(states[i])\n",
    "                td_error = returns[i] - state_value  # TD error\n",
    "                value_loss = td_error.pow(2).mean()  # Mean squared error\n",
    "                self.optimizer_value.zero_grad()\n",
    "                value_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.value_net.parameters(), 1.0)\n",
    "                self.optimizer_value.step()\n",
    "\n",
    "            # Subtract baseline (state value) from returns\n",
    "            baselines = torch.cat([self.value_net(s).detach() for s in states])\n",
    "            returns -= baselines\n",
    "\n",
    "        # Compute policy loss\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * R)  # Negative for gradient ascent\n",
    "\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "\n",
    "        # Update policy network\n",
    "        self.optimizer_policy.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        self.optimizer_policy.step()\n",
    "\n",
    "    def _compute_returns(self, rewards):\n",
    "        \"\"\"\n",
    "        Compute discounted returns G_t.\n",
    "        \"\"\"\n",
    "        G = 0\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.hp['gamma'] * G\n",
    "            returns.insert(0, G)\n",
    "\n",
    "        returns = torch.FloatTensor(returns)\n",
    "\n",
    "        if not self.hp['use_baseline'] and len(returns) > 1:  # Normalize only if no baseline is used\n",
    "            returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "\n",
    "        return returns\n",
    "\n",
    "    def train(self , wandb):\n",
    "        flag = True\n",
    "        for episode in range(self.hp['n_episodes']):\n",
    "            state, _ = self.env.reset()\n",
    "            log_probs = []\n",
    "            rewards = []\n",
    "\n",
    "            states = [] if self.hp['use_baseline'] else None\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                action, log_prob = self.get_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                log_probs.append(log_prob)\n",
    "                rewards.append(reward)\n",
    "\n",
    "                if states is not None:\n",
    "                    states.append(torch.FloatTensor(np.array(state)))\n",
    "\n",
    "                state = next_state\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if done:\n",
    "                    self.reward_buffer.append(sum(rewards))\n",
    "                    self.episode_reward.append(sum(rewards))\n",
    "                    # print(\"REWARDS\")\n",
    "                    # print(rewards)\n",
    "                    # mean_episode_reward = sum(rewards)/len(rewards)\n",
    "                    # print(mean_episode_reward)\n",
    "                    # wandb.log({\"episodic_reward\": mean_episode_reward})\n",
    "                    break\n",
    "\n",
    "            # Update policy (and baseline if applicable)\n",
    "            if flag:\n",
    "                self.update_policy(rewards, log_probs, states if states is not None else None)\n",
    "    \n",
    "                # Apply learning rate decay if configured\n",
    "                if self.scheduler_policy is not None:\n",
    "                    self.scheduler_policy.step()\n",
    "                if self.hp['use_baseline'] and self.scheduler_value is not None:\n",
    "                    self.scheduler_value.step()\n",
    "\n",
    "            # Logging\n",
    "            mean_episode_reward = sum(rewards)\n",
    "            wandb.log({\"episodic_reward\": mean_episode_reward})\n",
    "            if (episode + 1) % 10 == 0:\n",
    "                avg_reward = np.mean(self.reward_buffer)\n",
    "                self.avg_reward_list.append(avg_reward)\n",
    "                \n",
    "                print(f\"Episode {episode+1}: Average Reward (last 100) = {avg_reward:.2f}\")\n",
    "                wandb.log({\"mean_score_over_last_100_episodes\": avg_reward})\n",
    "\n",
    "                if avg_reward >= self.env.spec.reward_threshold:\n",
    "                    print(f\"Solved in {episode+1} episodes!\")\n",
    "                    flag = False\n",
    "                else:\n",
    "                    print(\"Reinitializing Training\")\n",
    "                    flag = True\n",
    "\n",
    "        return self.episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done the implemnation of the `REINFORCEAgent` above. Let's run the hyperparameter search to find the best hyperparameter for both the environments. In order to run hyperparameter search, we will be using `wandb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T09:13:29.697125Z",
     "iopub.status.busy": "2025-04-14T09:13:29.696721Z",
     "iopub.status.idle": "2025-04-14T09:13:29.718909Z",
     "shell.execute_reply": "2025-04-14T09:13:29.717932Z",
     "shell.execute_reply.started": "2025-04-14T09:13:29.697098Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_config_group_name(config):\n",
    "    return (\n",
    "        f\"{config.env_name}_\"\n",
    "        f\"{config.experiment}_\"\n",
    "        f\"BL{config.baseline}_\"\n",
    "        f\"neps{config.n_episodes}_\"\n",
    "        f\"lr{config.lr}_\"\n",
    "        f\"lrv{config.lr_value}_\"\n",
    "        f\"lsz{config.layer_size}_\"\n",
    "        f\"lyd{config.lr_decay}_\"\n",
    "        f\"gm{config.gamma}_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use this code to login to wandb if you are using it locally ensure that you have logined to wandb.\n",
    "#Don't run this code block if you are not using kaggle to execute the code.\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api = user_secrets.get_secret(\"wandb_api\")\n",
    "\n",
    "wandb.login(key = api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_wandb(config=None):\n",
    "    run = wandb.init(\n",
    "        project= \"DA6400 Assignment 2\",\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    # adding an attribute to group various runs with the same configuration\n",
    "    config = wandb.config\n",
    "    config_group = create_config_group_name(config)\n",
    "    wandb.config.update({\"config_group\": config_group}, allow_val_change=True)\n",
    "\n",
    "    wandb.run.name = name=f\"{config_group}_run_{uuid.uuid4().hex[:4]}\"\n",
    "\n",
    "    # initalise environment - passed as config parameter\n",
    "    env = gym.make(config.env_name)\n",
    "    HYPERPARAMS= {\n",
    "    'n_episodes': config.n_episodes,\n",
    "    'gamma': config.gamma,\n",
    "    'lr': config.lr,\n",
    "    'lr_value': config.lr_value,\n",
    "    'use_baseline': config.baseline,  # Enable baseline version\n",
    "    'layer_sizes':config.layer_size,  # Different sizes for each layer\n",
    "    'lr_decay': config.lr_decay      # Learning rate decay factor\n",
    "}\n",
    "    agent  = REINFORCEAgent(env, HYPERPARAMS)\n",
    "    rewards = agent.train(wandb = run )\n",
    "    run.log( {\"mean_total_reward\": sum(rewards)/len(rewards)})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have define the one specific sweep config for one of the environment. we will modify below code to run the sweep for all the environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\":\"grid\",\n",
    "    \"name\" : \"Hyperparmeter training with Cartpole, MCReinforce\",\n",
    "    \"metric\": {\n",
    "        \"goal\":\"maximize\",\n",
    "        \"name\":\"mean_total_reward\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"env_name\":{\n",
    "            \"value\":\"CartPole-v1\"\n",
    "        },\n",
    "        \"experiment\":{\n",
    "            \"value\":\"MCReinforce\"\n",
    "        },\n",
    "        \"baseline\":{\n",
    "            \"values\":[True, False]\n",
    "        },\n",
    "        \"n_episodes\":{\n",
    "            \"value\":2000\n",
    "        },\n",
    "        \"layer_size\":{\n",
    "            \"values\":[[128],[64],[64,64],[32,32]]\n",
    "        },\n",
    "        \"gamma\":{\n",
    "            \"values\":[0.99]\n",
    "        },\n",
    "        \"lr\":{\n",
    "            \"values\":[0.001, 0.0001 , 0.01]\n",
    "        },\n",
    "        \"lr_value\":{\n",
    "            \"values\": [0.001]\n",
    "        },\n",
    "        \"lr_decay\":{\n",
    "            \"values\":[1, 0.995 , 0.9995]\n",
    "        }\n",
    "        \n",
    "       \n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"DA6400 Assignment 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    wandb.agent(sweep_id, train_wandb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Run the above 3 cell repeatly for all your hyperparameter combinations. The code will run the training for each combination of hyperparameters and log the results to wandb. You can then visualize the results in the wandb dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once We have the best hyperparameters, we can run the experiments to genearte required visualisation in `wandb` dashboard. "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
