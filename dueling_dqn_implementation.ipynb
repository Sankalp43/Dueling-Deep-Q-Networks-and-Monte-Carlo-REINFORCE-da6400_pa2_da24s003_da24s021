{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-12T12:17:45.540054Z",
     "iopub.status.busy": "2025-04-12T12:17:45.539797Z",
     "iopub.status.idle": "2025-04-12T12:18:06.830967Z",
     "shell.execute_reply": "2025-04-12T12:18:06.830269Z",
     "shell.execute_reply.started": "2025-04-12T12:17:45.540036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium -qU\n",
    "!pip install wandb -qU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:18:06.833346Z",
     "iopub.status.busy": "2025-04-12T12:18:06.833097Z",
     "iopub.status.idle": "2025-04-12T12:18:12.903738Z",
     "shell.execute_reply": "2025-04-12T12:18:12.903151Z",
     "shell.execute_reply.started": "2025-04-12T12:18:06.833326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "from scipy.special import softmax\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the programming assignment 2, we are required to implement two alogrithms, `Monte Carlo - REINFORCE` and `Dueling Deep Q-learning Network (Dueling DQN)`. After wards, the goal is to run hyperparameter search to find the best hyperparameters for both algorithms on two environments, `CartPole-v1` and `Acrobot-v2`. \n",
    "\n",
    "In this jupyter notebook let's implement the `Dueling DQN` algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:18:12.904578Z",
     "iopub.status.busy": "2025-04-12T12:18:12.904377Z",
     "iopub.status.idle": "2025-04-12T12:18:18.948709Z",
     "shell.execute_reply": "2025-04-12T12:18:18.948162Z",
     "shell.execute_reply.started": "2025-04-12T12:18:12.904554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use this code to login to wandb if you are using it locally ensure that you have logined to wandb.\n",
    "#Don't run this code block if you are not using kaggle to execute the code.\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api = user_secrets.get_secret(\"wandb_api\")\n",
    "\n",
    "wandb.login(key = api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:18:18.949746Z",
     "iopub.status.busy": "2025-04-12T12:18:18.949390Z",
     "iopub.status.idle": "2025-04-12T12:18:19.013901Z",
     "shell.execute_reply": "2025-04-12T12:18:19.013301Z",
     "shell.execute_reply.started": "2025-04-12T12:18:18.949727Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:18:19.015220Z",
     "iopub.status.busy": "2025-04-12T12:18:19.014930Z",
     "iopub.status.idle": "2025-04-12T12:18:19.029156Z",
     "shell.execute_reply": "2025-04-12T12:18:19.028588Z",
     "shell.execute_reply.started": "2025-04-12T12:18:19.015197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DuelingQNetwork(nn.Module):\n",
    "    def __init__(self,state_space_size,action_space_size,num_layer,layer_size,value_fn_layer_size,advantage_fn_layer_size,algo_type='Type1'):\n",
    "\n",
    "        super().__init__()\n",
    "        self.algo_type = algo_type\n",
    "        if isinstance(layer_size,int):\n",
    "          layer_size = [layer_size] * num_layer\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(state_space_size, layer_size[0]))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        for i in range(1,num_layer):\n",
    "          layers.append(nn.Linear(layer_size[i-1],layer_size[i]))\n",
    "          layers.append(nn.ReLU())\n",
    "\n",
    "        self.state_approximator = nn.Sequential(\n",
    "            *layers\n",
    "        )\n",
    "\n",
    "        self.value_fn_network = nn.Sequential(\n",
    "            nn.Linear(layer_size[-1],value_fn_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(value_fn_layer_size,1)\n",
    "        )\n",
    "\n",
    "        self.advantage_fn_network = nn.Sequential(\n",
    "            nn.Linear(layer_size[-1],advantage_fn_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(advantage_fn_layer_size,action_space_size)\n",
    "        )\n",
    "\n",
    "    def forward(self,state_obs):\n",
    "        if len(state_obs.shape) == 1:\n",
    "          state_obs = state_obs.unsqueeze(0)\n",
    "\n",
    "        state_approximator = self.state_approximator(state_obs)\n",
    "        value_fn = self.value_fn_network(state_approximator)\n",
    "        advantage_fn = self.advantage_fn_network(state_approximator)\n",
    "\n",
    "        if self.algo_type == 'Type1':\n",
    "            q_value = value_fn + (advantage_fn - torch.mean(advantage_fn,dim = 1,keepdim=True))\n",
    "        elif self.algo_type =='Type2':\n",
    "            q_value = value_fn+(advantage_fn-torch.max(advantage_fn,dim=1,keepdim=True).values)\n",
    "\n",
    "        return q_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:18:19.029945Z",
     "iopub.status.busy": "2025-04-12T12:18:19.029781Z",
     "iopub.status.idle": "2025-04-12T12:18:19.046927Z",
     "shell.execute_reply": "2025-04-12T12:18:19.046282Z",
     "shell.execute_reply.started": "2025-04-12T12:18:19.029932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "  def __init__(self,batch_size,buffer_size,seed):\n",
    "    self.seed = random.seed(seed)\n",
    "    self.batch_size = batch_size\n",
    "    self.memory = deque(maxlen=buffer_size)\n",
    "    self.experience = namedtuple(\"Experience\",field_names=['state','action','reward','next_state','terminated','truncated'])\n",
    "\n",
    "  def add(self,state,action,reward,next_state,terminated,truncated):\n",
    "    experience = self.experience(state,action,reward,next_state,terminated,truncated)\n",
    "    self.memory.append(experience)\n",
    "\n",
    "  def sample(self):\n",
    "    experiences = random.sample(self.memory,self.batch_size)\n",
    "    states = torch.from_numpy(np.vstack([e.state for e in experiences])).float().to(device)\n",
    "    action = torch.from_numpy(np.vstack([e.action for e in experiences])).long().to(device)\n",
    "    reward = torch.from_numpy(np.vstack([e.reward for e in experiences])).float().to(device)\n",
    "    next_state = torch.from_numpy(np.vstack([e.next_state for e in experiences])).float().to(device)\n",
    "    terminated = torch.from_numpy(np.vstack([e.terminated for e in experiences]).astype(np.uint8)).float().to(device)\n",
    "    truncated = torch.from_numpy(np.vstack([e.truncated for e in experiences]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "    return (states,action,reward,next_state,terminated,truncated)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:29:27.251414Z",
     "iopub.status.busy": "2025-04-12T12:29:27.251096Z",
     "iopub.status.idle": "2025-04-12T12:29:27.256712Z",
     "shell.execute_reply": "2025-04-12T12:29:27.255919Z",
     "shell.execute_reply.started": "2025-04-12T12:29:27.251393Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxPolicy:\n",
    "  def __init__(self,tau,tau_decay, min_tau):\n",
    "    self.tau = tau\n",
    "    self.tau_decay = tau_decay\n",
    "    self.min_tau = min_tau\n",
    "\n",
    "  def decay_policy_param(self):\n",
    "    self.tau = max(self.min_tau,self.tau * self.tau_decay)\n",
    "\n",
    "  def get_action(self, q_values):\n",
    "    q_values = q_values.detach().cpu().numpy().ravel()\n",
    "    logits = q_values - np.max(q_values)\n",
    "    prob = softmax(logits/self.tau)\n",
    "    action = np.random.choice(np.arange(len(q_values)),p=prob)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:18:19.065363Z",
     "iopub.status.busy": "2025-04-12T12:18:19.065146Z",
     "iopub.status.idle": "2025-04-12T12:18:19.077345Z",
     "shell.execute_reply": "2025-04-12T12:18:19.076732Z",
     "shell.execute_reply.started": "2025-04-12T12:18:19.065334Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EpsilonGreedyPolicy:\n",
    "  def __init__(self,epsilon,epsilon_decay,min_epsilon):\n",
    "    self.epsilon = epsilon\n",
    "    self.epsilon_decay = epsilon_decay\n",
    "    self.min_epsilon = min_epsilon\n",
    "\n",
    "  def decay_policy_param(self):\n",
    "    self.epsilon = max(self.min_epsilon,self.epsilon * self.epsilon_decay)\n",
    "\n",
    "  def get_action(self,q_values):\n",
    "    q_values = q_values.detach().cpu().numpy()\n",
    "    if random.random() < self.epsilon:\n",
    "      action = np.random.choice(np.arange(len(q_values)))\n",
    "    else:\n",
    "      action = np.argmax(q_values)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have done the implemnation of the `Dueling DQN` above. Let's run the hyperparameter search to find the best hyperparameter for both the environments. In order to run hyperparameter search, we will be using `wandb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:18:19.078138Z",
     "iopub.status.busy": "2025-04-12T12:18:19.077975Z",
     "iopub.status.idle": "2025-04-12T12:18:19.094894Z",
     "shell.execute_reply": "2025-04-12T12:18:19.094281Z",
     "shell.execute_reply.started": "2025-04-12T12:18:19.078125Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_config_group_name(config):\n",
    "    return (\n",
    "        f\"{config.env_name}_\"\n",
    "        f\"{config.experiment}_\"\n",
    "        f\"{config.type}_\"\n",
    "        f\"nl{config.num_layer}_\"\n",
    "        f\"ls{config.layer_size}_\"\n",
    "        f\"vf{config.value_fn_layer_size}_\"\n",
    "        f\"af{config.advantage_fn_layer_size}_\"\n",
    "        f\"freq{config.target_network_replacement_freq}_\"\n",
    "        f\"lr{config.learning_rate:.0e}_\"\n",
    "        f\"{config.exploration_policy}_\"\n",
    "        f\"decay{config.param_decay:.4f}_\"\n",
    "        f\"p{config.policy_param}_\"\n",
    "        f\"pmin{config.policy_param_min_value}_\"\n",
    "        f\"batch{config.batch_size}_\"\n",
    "        f\"gamma{config.gamma}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:52:56.752806Z",
     "iopub.status.busy": "2025-04-12T12:52:56.752532Z",
     "iopub.status.idle": "2025-04-12T12:52:56.765067Z",
     "shell.execute_reply": "2025-04-12T12:52:56.764440Z",
     "shell.execute_reply.started": "2025-04-12T12:52:56.752787Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(config=None):\n",
    "    max_episodes = 1000\n",
    "    buffer_size = 10000\n",
    "    seed = 200\n",
    "    \n",
    "    wandb.init(\n",
    "        project= \"DA6400 Assignment 2\",\n",
    "        config = config\n",
    "    )\n",
    "\n",
    "    # adding an attribute to group various runs with the same configuration\n",
    "    config = wandb.config\n",
    "    config_group = create_config_group_name(config)\n",
    "    wandb.config.update({\"config_group\": config_group}, allow_val_change=True)\n",
    "\n",
    "    wandb.run.name = name=f\"{config_group}_run_{uuid.uuid4().hex[:4]}\"\n",
    "\n",
    "    # initalise environment - passed as config parameter\n",
    "    env = gym.make(config.env_name)\n",
    "    state_space_size = env.observation_space.shape[0]\n",
    "    action_space_size = env.action_space.n\n",
    "    \n",
    "    #initalise replay buffer, local network, etc\n",
    "    memory = ReplayBuffer(config.batch_size,buffer_size,seed)\n",
    "    local_network = DuelingQNetwork(state_space_size,\n",
    "                                   action_space_size,\n",
    "                                   config.num_layer,\n",
    "                                   config.layer_size,\n",
    "                                   config.value_fn_layer_size,\n",
    "                                   config.advantage_fn_layer_size,\n",
    "                                   config.type).to(device)\n",
    "    target_network = DuelingQNetwork(state_space_size,\n",
    "                                   action_space_size,\n",
    "                                   config.num_layer,\n",
    "                                   config.layer_size,\n",
    "                                   config.value_fn_layer_size,\n",
    "                                   config.advantage_fn_layer_size,\n",
    "                                   config.type).to(device)\n",
    "    optimizer = optim.Adam(local_network.parameters(),lr = config.learning_rate)\n",
    "\n",
    "\n",
    "    #based on the config, initial the policy for action selection\n",
    "    if config.exploration_policy == 'EpsilonGreedy':\n",
    "        policy = EpsilonGreedyPolicy(config.policy_param,\n",
    "                                    config.param_decay,\n",
    "                                    config.policy_param_min_value)\n",
    "    elif config.exploration_policy == 'Softmax':\n",
    "        policy = SoftmaxPolicy(config.policy_param,\n",
    "                                    config.param_decay,\n",
    "                                    config.policy_param_min_value)\n",
    "    else:\n",
    "        raise ValueError(\"Policy not implemented\")\n",
    "\n",
    "    #Let's start the training and sampling\n",
    "    # we will use sample of replay buffer to train the network\n",
    "\n",
    "    scores_window = deque(maxlen = 100)\n",
    "    step_counter = 0\n",
    "    total_mean_reward = 0 # summary statistic that will be maximized instead of minimizing regret\n",
    "    mean_scores = 0\n",
    "    for episode in tqdm(range(max_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        done = False\n",
    "        episodic_reward = 0\n",
    "        while not done:\n",
    "            local_network.eval()\n",
    "            with torch.no_grad():\n",
    "                q_values = local_network(torch.from_numpy(state).to(device))\n",
    "            action = policy.get_action(q_values)\n",
    "            local_network.train()\n",
    "\n",
    "            next_state, reward,terminated,truncated, _ = env.step(action)\n",
    "            memory.add(state,action,reward,next_state,terminated,truncated)\n",
    "            done = terminated or truncated\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "            episodic_reward += reward\n",
    "\n",
    "            if len(memory) > config.batch_size and mean_scores < env.spec.reward_threshold:\n",
    "                #training steps\n",
    "                states,actions,rewards,next_states,terminated,truncated = memory.sample()\n",
    "                q_next = target_network(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "                q_targets = rewards + (1-terminated) * config.gamma * q_next\n",
    "\n",
    "                q_expected = local_network(states).gather(1,actions)\n",
    "                loss = F.mse_loss(q_expected,q_targets)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                for param in local_network.parameters():\n",
    "                    param.grad.data.clamp_(-1,1)\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "            step_counter = (step_counter + 1) % config.target_network_replacement_freq\n",
    "            if step_counter == 0:\n",
    "                target_network.load_state_dict(local_network.state_dict())\n",
    "        \n",
    "        policy.decay_policy_param()\n",
    "        scores_window.append(episodic_reward)\n",
    "\n",
    "        mean_scores = np.mean(scores_window)\n",
    "        wandb.log({\n",
    "            \"mean_score_over_last_100_episodes\": mean_scores,\n",
    "            \"episodic_reward\": episodic_reward\n",
    "        }) \n",
    "\n",
    "        total_mean_reward += (episodic_reward - total_mean_reward) / (episode + 1)\n",
    "\n",
    "        # if mean_scores >= env.spec.reward_threshold:\n",
    "        #     break\n",
    "\n",
    "            \n",
    "    wandb.log({\n",
    "        'Total Mean Reward' : total_mean_reward\n",
    "    })\n",
    "\n",
    "    wandb.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have define the one specific sweep config for one of the environment. we will modify below code to run the sweep for all the environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T01:23:19.756970Z",
     "iopub.status.busy": "2025-04-14T01:23:19.756672Z",
     "iopub.status.idle": "2025-04-14T01:23:19.766045Z",
     "shell.execute_reply": "2025-04-14T01:23:19.765265Z",
     "shell.execute_reply.started": "2025-04-14T01:23:19.756946Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\":\"Hyperparameter tuning for type 2 D2QN, cartpole\",\n",
    "    \"method\":\"grid\",\n",
    "    \"metric\": {\n",
    "        \"goal\":\"maximize\",\n",
    "        \"name\":\"mean_total_reward\"\n",
    "    },\n",
    "    \"parameters\":{\n",
    "        \"env_name\":{\n",
    "            \"value\":\"CartPole-v1\"\n",
    "        },\n",
    "        \"experiment\":{\n",
    "            \"value\":\"DuelingDQN\"\n",
    "        },\n",
    "        \"type\":{\n",
    "            \"value\":\"Type2\"\n",
    "        },\n",
    "        \"num_layer\":{\n",
    "            \"value\":2\n",
    "        },\n",
    "        \"layer_size\":{\n",
    "            \"value\":[128,64]\n",
    "        },\n",
    "        \"value_fn_layer_size\":{\n",
    "            \"value\":32\n",
    "        },\n",
    "        \"advantage_fn_layer_size\":{\n",
    "            \"value\":32\n",
    "        },\n",
    "        \"target_network_replacement_freq\":{\n",
    "            \"values\": [10,20,50]\n",
    "        },\n",
    "        \"learning_rate\":{\n",
    "            \"values\":[0.01,0.001,0.05]\n",
    "        },\n",
    "        \"gamma\":{\n",
    "            \"value\": 0.998\n",
    "        },\n",
    "        \"batch_size\":{\n",
    "            \"values\": [64,128]\n",
    "        },\n",
    "        \"exploration_policy\":{\n",
    "            \"values\": ['Softmax','EpsilonGreedy']\n",
    "        },\n",
    "        \"param_decay\":{\n",
    "            \"values\":[0.995,0.9995]\n",
    "        },\n",
    "        \"policy_param\":{\n",
    "            \"value\": 1\n",
    "        },\n",
    "        \"policy_param_min_value\":{\n",
    "            \"value\": 0.01\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T12:53:03.869627Z",
     "iopub.status.busy": "2025-04-12T12:53:03.869058Z",
     "iopub.status.idle": "2025-04-12T12:53:04.211412Z",
     "shell.execute_reply": "2025-04-12T12:53:04.210617Z",
     "shell.execute_reply.started": "2025-04-12T12:53:03.869598Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"DA6400 Assignment 2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-04-12T12:55:00.951Z",
     "iopub.execute_input": "2025-04-12T12:54:34.213096Z",
     "iopub.status.busy": "2025-04-12T12:54:34.212559Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.agent(sweep_id, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Run the above 3 cell repeatly for all your hyperparameter combinations. The code will run the training for each combination of hyperparameters and log the results to wandb. You can then visualize the results in the wandb dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once We have the best hyperparameters, we can run the experiments to genearte required visualisation in `wandb` dashboard. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to generate the plots, we can use the below code, define a dictionary with the hyperparameters and use that to run the experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "best_hyperparamters = {\n",
    "        \"env_name\":\"Acrobot-v1\",\n",
    "        \"experiment\":\"DuelingDQN\",\n",
    "        \"type\":\"Type2\",\n",
    "        \"num_layer\":2,\n",
    "        \"layer_size\":[128,64],\n",
    "        \"value_fn_layer_size\":32,\n",
    "        \"advantage_fn_layer_size\":32,\n",
    "        \"target_network_replacement_freq\": 50,\n",
    "        \"learning_rate\":0.001,\n",
    "        \"gamma\": 0.998,\n",
    "        \"batch_size\":128,\n",
    "        \"exploration_policy\": 'Softmax',\n",
    "        \"param_decay\":0.995,\n",
    "        \"policy_param\": 1,\n",
    "        \"policy_param_min_value\": 0.01,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    train(best_hyperparamters)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
